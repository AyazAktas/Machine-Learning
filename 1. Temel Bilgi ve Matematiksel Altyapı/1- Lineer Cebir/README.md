Lineer cebir, makine Ã¶ÄŸrenmesinin omurgasÄ±dÄ±r. Ã‡oÄŸu zaman modelin iÃ§inde gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z karmaÅŸÄ±k iÅŸlemler, aslÄ±nda satÄ±r satÄ±r lineer cebir operasyonlarÄ±na dÃ¶nÃ¼ÅŸÃ¼r. Åimdi bunlarÄ± ele alalÄ±m; kavramlarÄ± tek tek aÃ§alÄ±m, gÃ¼nlÃ¼k hayattan Ã¶rneklerle baÄŸlayalÄ±m ve neden makine Ã¶ÄŸrenmesinde bu kadar kritik olduklarÄ±nÄ± gÃ¶relim.

---

## ğŸ¯ VektÃ¶r

VektÃ¶r, bir sayÄ± listesidir ama sadece liste demek yetmez; bu listenin **bÃ¼yÃ¼klÃ¼ÄŸÃ¼** ve **yÃ¶nÃ¼** vardÄ±r.

ğŸ”¹ **Matematiksel tanÄ±m:**
Bir vektÃ¶r, $\mathbb{R}^n$ uzayÄ±nda tanÄ±mlanmÄ±ÅŸ, $n$ boyutlu bir sayÄ± dizisidir.
Ã–rn: $v = [2, 5, -1]$ Ã¼Ã§ boyutlu bir vektÃ¶rdÃ¼r.

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Market arabasÄ±nÄ± dÃ¼ÅŸÃ¼n. ArabayÄ± **3 Newton** kuvvetle ileri doÄŸru, **2 Newton** saÄŸa doÄŸru itiyorsun. Bu iki sayÄ± (3 ve 2) bir araya geldiÄŸinde senin â€œhareket ettirme vektÃ¶rÃ¼nâ€ oluÅŸuyor. Yani vektÃ¶r, sadece â€œkaÃ§â€ deÄŸil, aynÄ± zamanda â€œnereyeâ€ sorusunun da cevabÄ±dÄ±r.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**
Her veri noktasÄ±nÄ± bilgisayar aslÄ±nda bir vektÃ¶r olarak gÃ¶rÃ¼r.

* Bir evin fiyatÄ±nÄ± tahmin etmek istersek, evin metrekaresi, oda sayÄ±sÄ±, ÅŸehir merkezi uzaklÄ±ÄŸÄ±â€¦ hepsi birer sayÄ±. BunlarÄ± \[120, 3, 5] gibi vektÃ¶r haline getiririz.
* GÃ¶rsel iÅŸleme yaparken her pikselin RGB deÄŸeri de bir vektÃ¶rdÃ¼r.

---

## ğŸ¯ Matris

Matris, vektÃ¶rlerin tablosu gibi dÃ¼ÅŸÃ¼nÃ¼lebilir.

ğŸ”¹ **Matematiksel tanÄ±m:**
DikdÃ¶rtgen biÃ§iminde dÃ¼zenlenmiÅŸ sayÄ± tablosudur. Ã–rn:

$$
A = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
$$

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Bir okulun not defterini dÃ¼ÅŸÃ¼n. SatÄ±rlar Ã¶ÄŸrencileri, sÃ¼tunlar dersleri temsil etsin. Her hÃ¼creye Ã¶ÄŸrencinin aldÄ±ÄŸÄ± not yazÄ±lÄ±r. Bu tablo aslÄ±nda bir matristir.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* Veriler genellikle $n \times m$ boyutlu bir matriste saklanÄ±r. (n: Ã¶rnek sayÄ±sÄ±, m: Ã¶zellik sayÄ±sÄ±)
* Bir sinir aÄŸÄ±nda giriÅŸten Ã§Ä±kÄ±ÅŸa olan dÃ¶nÃ¼ÅŸÃ¼mler hep matris Ã§arpÄ±mlarÄ±yla yapÄ±lÄ±r.

---

## ğŸ¯ Determinant

Determinant, matrisin bize verdiÄŸi â€œÃ¶lÃ§ek faktÃ¶rÃ¼â€dÃ¼r.

ğŸ”¹ **Matematiksel tanÄ±m:**
Bir kare matrisin determinantÄ±, o matrisin alanÄ±/hacmi ne kadar bÃ¼yÃ¼tÃ¼p kÃ¼Ã§Ã¼lttÃ¼ÄŸÃ¼nÃ¼ gÃ¶sterir.

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Bir haritayÄ± bÃ¼yÃ¼tÃ¼p kÃ¼Ã§Ã¼lten fotokopi makinesini dÃ¼ÅŸÃ¼n. EÄŸer fotokopi %200 bÃ¼yÃ¼tÃ¼yorsa, senin kare kaÄŸÄ±dÄ±n artÄ±k 2 kat daha bÃ¼yÃ¼k olur. Determinant bu bÃ¼yÃ¼tme katsayÄ±sÄ±nÄ± sÃ¶yler. EÄŸer determinant 0 Ã§Ä±karsa, bu ÅŸu anlama gelir: matris uzayÄ± bir boyut â€œÃ§Ã¶kerterekâ€ sÄ±kÄ±ÅŸtÄ±rÄ±yor (yani bilgiyi kaybediyor).

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* Matrisin tersinin var olup olmadÄ±ÄŸÄ±nÄ± anlamak iÃ§in determinant kontrol edilir.
* BazÄ± algoritmalarda (Ã¶rneÄŸin Gaussian daÄŸÄ±lÄ±mlarÄ±nda kovaryans matrisinin determinantÄ±) doÄŸrudan hesaplara girer.

---

## ğŸ¯ Eigenvalue (Ã¶zdeÄŸer) ve Eigenvector (Ã¶zvektÃ¶r)

Bunlar lineer cebirin en bÃ¼yÃ¼lÃ¼ kavramlarÄ±ndandÄ±r.

ğŸ”¹ **TanÄ±m:**

* Bir matrisi vektÃ¶rle Ã§arptÄ±ÄŸÄ±nda, genelde yÃ¶n deÄŸiÅŸir. Ama bazÄ± Ã¶zel vektÃ¶rler vardÄ±r ki, yÃ¶nleri deÄŸiÅŸmez; sadece uzunluklarÄ± farklÄ±laÅŸÄ±r. Ä°ÅŸte bu Ã¶zel vektÃ¶rler **Ã¶zvektÃ¶r**, uzunluklarÄ±nÄ± ne kadar deÄŸiÅŸtirdiÄŸini sÃ¶yleyen katsayÄ± da **Ã¶zdeÄŸer**dir.

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Bir ÅŸirketin iÅŸleyiÅŸini dÃ¼ÅŸÃ¼n. Åirket her sene yeniden yapÄ±lanÄ±yor, ama bazÄ± departmanlar (mesela muhasebe) aynÄ± dÃ¼zeniyle kalÄ±yor; sadece Ã§alÄ±ÅŸan sayÄ±sÄ± artÄ±yor veya azalÄ±yor. Bu deÄŸiÅŸmeyen â€œyÃ¶nâ€ â†’ Ã¶zvektÃ¶r, deÄŸiÅŸim katsayÄ±sÄ± â†’ Ã¶zdeÄŸer.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* PCA (Principal Component Analysis) â†’ boyut indirgeme yÃ¶nteminde, veri daÄŸÄ±lÄ±mÄ±ndaki en gÃ¼Ã§lÃ¼ yÃ¶nleri bulmak iÃ§in Ã¶zvektÃ¶r/Ã¶zdeÄŸerler kullanÄ±lÄ±r.
* Grafiklerde en baskÄ±n yÃ¶nÃ¼, varyansÄ± aÃ§Ä±klayan eksenleri Ã§Ä±karÄ±r.

---

## ğŸ¯ SVD (Singular Value Decomposition)

SVD, veriyi Ã¼Ã§ parÃ§aya ayÄ±ran gÃ¼Ã§lÃ¼ bir faktÃ¶rizasyon tekniÄŸidir.

ğŸ”¹ **TanÄ±m:**
Her matrisi Ã¼Ã§ matrisin Ã§arpÄ±mÄ± olarak yazabiliriz:

$$
A = U \Sigma V^T
$$

* $U$: SatÄ±r yÃ¶nlerini temsil eder.
* $\Sigma$: Tekil deÄŸerler (en Ã¶nemli bilgiler burada).
* $V^T$: SÃ¼tun yÃ¶nlerini temsil eder.

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Bir mÃ¼ziÄŸi dÃ¼ÅŸÃ¼n. MÃ¼ziÄŸi Ã¼Ã§e ayÄ±rabilirsin: vokal, ritim, enstrÃ¼man. Bunlar birleÅŸince orijinal mÃ¼zik ortaya Ã§Ä±kar. SVD de bir matrisi bu bileÅŸenlerine ayÄ±rÄ±r.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* PCA aslÄ±nda SVDâ€™nin Ã¶zel bir kullanÄ±mÄ±dÄ±r.
* Ã–neri sistemlerinde (Netflix, Spotify) kullanÄ±cÄ±â€“film matrisini faktÃ¶rleyip gizli Ã¶zellikleri Ã§Ä±karÄ±rÄ±z.
* GÃ¼rÃ¼ltÃ¼ azaltma ve sÄ±kÄ±ÅŸtÄ±rmada da Ã§ok kullanÄ±lÄ±r.

---

### ğŸ”‘ Ã–zet

* **VektÃ¶r** â†’ Ã¶zelliklerimizi temsil eden sayÄ± dizileri.
* **Matris** â†’ verilerin tablolaÅŸmÄ±ÅŸ hali.
* **Determinant** â†’ matrisin Ã¶lÃ§ekleme gÃ¼cÃ¼.
* **Eigenvalue/Eigenvector** â†’ verideki en baskÄ±n yÃ¶nleri keÅŸfetmek.
* **SVD** â†’ matrisleri parÃ§alayarak gizli yapÄ±larÄ± ortaya Ã§Ä±karmak.

Makine Ã¶ÄŸrenmesinde bu kavramlar olmadan ilerlemek mÃ¼mkÃ¼n deÄŸil. Ã‡Ã¼nkÃ¼ algoritmalarÄ±n iÃ§ini aÃ§tÄ±ÄŸÄ±mÄ±zda gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z ÅŸey, aslÄ±nda hep bu yapÄ± taÅŸlarÄ±nÄ±n matematiksel dansÄ±.

---
