TÃ¼rev, integral ve zincir kuralÄ± denince kulaÄŸa hemen aÄŸÄ±r matematik gibi geliyor ama aslÄ±nda gÃ¼nlÃ¼k hayatta hepimizin yaÅŸadÄ±ÄŸÄ± ÅŸeylerin sayÄ±sal ifadesinden ibaret. BunlarÄ± anladÄ±ÄŸÄ±nda Gradient Descent gibi â€œmakine Ã¶ÄŸrenmesinin kalbiâ€ olan algoritmalarÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± da kavramak kolaylaÅŸacak. Hadi adÄ±m adÄ±m gidelim.

---

## ğŸ¯ TÃ¼rev (Derivative)

TÃ¼rev, bir ÅŸeyin anlÄ±k deÄŸiÅŸim hÄ±zÄ±nÄ± Ã¶lÃ§er.

ğŸ”¹ **TanÄ±m (matematiksel):**
Bir fonksiyonun belli bir noktadaki eÄŸimidir.

$$
f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}
$$

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Araba sÃ¼rÃ¼yorsun, hÄ±z gÃ¶stergesinde 90 km/s yazÄ±yor. Bu aslÄ±nda tÃ¼revdir. Ã‡Ã¼nkÃ¼ tÃ¼rev, â€œkonumun zamana gÃ¶re deÄŸiÅŸim hÄ±zÄ±â€dÄ±r â†’ yani hÄ±z.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* Bir modelin hata fonksiyonunu dÃ¼ÅŸÃ¼n (loss function). Bizim amacÄ±mÄ±z bu hatayÄ± kÃ¼Ã§Ã¼ltmek. Hata fonksiyonunun tÃ¼revi bize â€œÅŸu an bu noktada hata hangi yÃ¶ne doÄŸru azalÄ±yor?â€ sorusunun cevabÄ±nÄ± verir.
* Yani tÃ¼rev = â€œhata fonksiyonunun eÄŸimi â†’ hangi yÃ¶nde adÄ±m atmalÄ±yÄ±m?â€

---

## ğŸ¯ Zincir KuralÄ± (Chain Rule)

Bazen fonksiyonlar iÃ§ iÃ§e geÃ§miÅŸ olur. O zaman tÃ¼rev almak iÃ§in zincir kuralÄ±na ihtiyaÃ§ duyarÄ±z.

ğŸ”¹ **Matematiksel ifade:**
EÄŸer $y = f(g(x))$ ise:

$$
\frac{dy}{dx} = f'(g(x)) \cdot g'(x)
$$

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
Diyelim ki ben sana her gÃ¼n kahve Ä±smarlÄ±yorum. Kahve iÃ§mek â†’ seni enerjik yapÄ±yor. Enerjik olman â†’ sÄ±nav notunu yÃ¼kseltiyor.

* Kahve miktarÄ± â†’ enerji (1. iliÅŸki)
* Enerji â†’ sÄ±nav notu (2. iliÅŸki)

Sen â€œkahve miktarÄ± sÄ±nav notunu nasÄ±l etkiler?â€ diye sorarsan bu iki etkiyi zincir kuralÄ±yla Ã§arparÄ±z.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* Sinir aÄŸlarÄ±nda ileri doÄŸru iÅŸlemde aktivasyonlar iÃ§ iÃ§e fonksiyonlardÄ±r.
* Geriye yayÄ±lÄ±m (backpropagation) tamamen zincir kuralÄ± ile tÃ¼revleri hesaplayÄ±p hatayÄ± katman katman geriye yaymaktan ibarettir.

---

## ğŸ¯ Ä°ntegral (Integral)

Ä°ntegral, tÃ¼revin tersidir. KÃ¼Ã§Ã¼k parÃ§alarÄ± toplayÄ±p bÃ¼yÃ¼k resmi gÃ¶rmeyi saÄŸlar.

ğŸ”¹ **TanÄ±m (matematiksel):**
Bir fonksiyonun altÄ±ndaki alan.

$$
\int f(x) dx
$$

ğŸ”¹ **GÃ¼nlÃ¼k hayat Ã¶rneÄŸi:**
HÄ±z gÃ¶stergesinden anlÄ±k hÄ±zlarÄ± biliyorsun. Ama â€œben toplamda kaÃ§ kilometre yol yaptÄ±m?â€ dersen iÅŸte integral devreye girer. HÄ±zÄ± (kÃ¼Ã§Ã¼k parÃ§alar halinde) toplarsÄ±n â†’ toplam yol Ã§Ä±kar.

ğŸ”¹ **Makine Ã¶ÄŸrenmesinde yeri:**

* OlasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ±nda â€œbu aralÄ±kta deÄŸer alma ihtimali nedir?â€ sorusu integralle hesaplanÄ±r.
* Continuous (sÃ¼rekli) modellerde alan hesaplamak integrale baÄŸlÄ±dÄ±r.

---

## ğŸ¯ Gradient Descent ile BaÄŸlantÄ±

Gradient Descent (Gradyan Ä°niÅŸi) â†’ en temel optimizasyon yÃ¶ntemi.

ğŸ”¹ **MantÄ±k:**
Bir daÄŸÄ±n tepesindesin ve gÃ¶zlerin kapalÄ±. AmacÄ±n en kÄ±sa yoldan aÅŸaÄŸÄ± inmek. Elinle yokluyorsun, eÄŸimin en fazla hangi yÃ¶nde olduÄŸunu hissediyorsun, o yÃ¶ne kÃ¼Ã§Ã¼k bir adÄ±m atÄ±yorsun. Sonra tekrar bakÄ±yor, tekrar adÄ±m atÄ±yorsun. Ä°ÅŸte Gradient Descent bu.

FormÃ¼l:

$$
\theta_{new} = \theta_{old} - \alpha \cdot \nabla J(\theta)
$$

* $\theta$: model parametreleri
* $J(\theta)$: hata fonksiyonu (loss)
* $\nabla J(\theta)$: gradyan (tÃ¼revlerle hesaplanan eÄŸim)
* $\alpha$: Ã¶ÄŸrenme oranÄ± (learning rate)

ğŸ”¹ **BaÄŸlantÄ±lar:**

* **TÃ¼rev** â†’ EÄŸim bilgisini verir, hangi yÃ¶nde gitmeliyim?
* **Zincir kuralÄ±** â†’ Derin aÄŸlarda bu tÃ¼revleri katman katman hesaplamamÄ± saÄŸlar.
* **Ä°ntegral** â†’ DaÄŸÄ±lÄ±mlar ve alan hesaplamalarÄ±nda destekleyici ama Gradient Descentâ€™te doÄŸrudan deÄŸil, yan rol oynar.

---

### ğŸ”‘ Ã–zet

* TÃ¼rev = anlÄ±k deÄŸiÅŸim hÄ±zÄ± â†’ yÃ¶n bulma.
* Zincir kuralÄ± = iÃ§ iÃ§e fonksiyonlarÄ±n tÃ¼revini Ã§Ã¶zme â†’ backpropagation.
* Ä°ntegral = kÃ¼Ã§Ã¼k parÃ§alarÄ± toplama â†’ olasÄ±lÄ±k alanÄ±.
* Gradient Descent = tÃ¼revlerin verdiÄŸi eÄŸimi takip ederek hatayÄ± adÄ±m adÄ±m azaltma.

---
